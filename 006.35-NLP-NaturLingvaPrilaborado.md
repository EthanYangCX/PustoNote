

CLC: TP391

# BERT

- 下载 BERT


# 数学之美

- 1 文字和语言vs数字和信息：香农发明信息论后方联系数学和信息系统
  - 1.1 信息：产生、传播、接收、反馈，`信息传播模型`古今无异
  - 1.2 文字和数字：同符异义ﾊ概念的概括和归类，似于`聚类`，用上下文`消歧`；翻译之可行，唯因不同的文字系统记录信息的能力等价；罗塞塔石碑的启示——`冗余`保障信息安全，`语料`（语言ﾉ数据）尤其对照语料对翻译至要；`数字`、`计数系统`，`编码`
  - 1.3 文字和语言背后的数学：文字设计（编码方法）符合`最短编码原理`；书面的窄`信道`（须压缩）与口头的宽信道；圣经的`校验码`；词法语法ﾊ编码规则，前者完备后者不；语言（真实语料）与语法（规则），NLP宣布前者胜；
  - 1.4 总结：
    - 后面重点：通信的原理和信息传播的模型；；（信源）编码和最短编码；；解码的规则，语法；；聚类；；校验位；；双语对照文本，语料库和机器翻译；；多义性和利用上下文消歧义
    - 祖先解决它们，多自发而非自觉，与今同者ﾊ数学规律
- 2 自然语言处理——从规则到统计
  - 2.1 机器智能：图灵测试；；50~70年代基于规则，“鸟飞派”，当时理解NLP——①（基础层）句法分析、语义分析②（认知层）自然语言理解③（应用层）语音识别机器翻译自动问答自动摘要；；句法分析——`语法分析树`，其文法规则——`重写规则`；；（基于乔姆斯基形式语言的编译器技术）；；过不去的坎——①文法规则太多②上下文有关；；
  - 2.2 从规则到统计：争议15年；；
  - 2.3 小结：统计NLP与通信的数学模型相通甚至相同
- 3 统计语言模型：为自然语言的上下文相关特性建立数模
  - 3.1 用数学的方法描述语言规律：贾里尼克ﾊ简单的统计模型，S'马尔可夫假设（由柯尔莫果洛夫一般化到可数无限状态空间） X'复杂的条件概率 c'简单，即`二元模型（Bigram Model）`， 以语料库中的频度算概率
  - 3.2 延伸阅读——统计语言模型的工程诀窍：
    - 高阶语言模型：一般三元，谷歌四元，大跨度要用长程依赖性
    - 模型的训练、零概率问题和平滑方法：`模型的训练`指从语料得条件概率；；用采样数据预测概率的原理是`大数定律`，要求观测值足够；；数据量恒不够，直算总有很多条件概率零，称模型`不平滑`；；`古德-图灵估计（Good-Turing Estimate）`，重估概率，对不可信的数据打折扣，概率的总量中分一部给未见事件；$d_r = (r+1) · N_{r+1} / N_r$；；实际自语处中次数超阈值词频率不下调，低于者总的下调频率给未出现词；；多元者`卡茨退避法（Katz backoff）`；；昔用低阶语言模型和高阶模型线性插值来平滑，称`删除插值`，略逊
    - 语料的选取问题：纵有噪、错，也应，训练语料和模型应用的领域一致；；训练数据常越多越好；；噪音高低也要考虑，有时要预处理，能找到模式、量大的噪音更应过滤
  - 3.3 数学的魅力在于将复杂的问题简单化
- 4 谈谈中文分词
  - 1 中文分词方法的演变：查词典（梁南元），最少词数分词理论（王晓龙）；；统计语言模型（郭进）（找概率最大，不是穷举，而是动态规划，维特比（Viterbi）算法），后来，没有词典的分词（孙茂松），用于英文词组（吴德凯）；；分词的同时找到复合词的嵌套结构，以解决粒度（郭进）
  - 2 延伸阅读——工程细节上的问题：
    - 2.1 分词的一致性：运用统计，不同分词器的差异远小于不同人的差异；；所幸中文分词是一个已经解决的问题，提高的空间微乎其微，只要采用统计语言模型，效果都不咋差
    - 2.2 词的颗粒度和层次：机器翻译要颗粒度大，网页搜索要小；；一个分词器同时支持不同层次的切分，需要一个基本词表和一个复合词表，各建立一个语言模型，同样的程序分词先基后复；；分词的不一致细分为错误（越界型错误、覆盖型错误）、颗粒度不一致；；近年来中文分词主要花精力的地方，是做数据挖掘的工作，不断完善复合词词典
  - 3 小结：基本看作已经解决的问题；；不同人的分词器差别主要在数据的使用和工程实现的精度
- 5 隐含马尔可夫模型
  - 5.1 通信模型：雅各布森通信六要素；；通过观测信息o,,，找最可能产生之的源信息s,,，即s,,使P(s,,|o,,)（后验）最大；；用贝叶斯公式(化为先验概率乘以似然度除以标准化常量)，其用隐马；；
  - 5.2 隐含马尔可夫模型：马尔可夫ﾊ随机过程を简化；；隐马是任意时刻状态st不可见，无法用st的状态序列估计参数，但每时刻输出与且仅与st相关的符号ot（谓独立输出假设），而隐含的状态s,,是马链；；根据马尔可夫假设和独立输出假设，通信模型的贝叶斯公式可化为隐马模型的求解公式了；；P(s,,)谓语言模型，P(s,,|o,,)（等式左）根据应用有不同名称
  - 5.3 延伸阅读——隐马模型的训练：
    - 隐马三个基本问题：①已知模型求某输出序列概率（前向后向算法）②已知模型和输出序列求最可能产生之的状态序列（维特比算法）③已知足够量观测数据估计隐马模型参数（即模型训练，鲍姆-韦尔奇算法）
    - 问题三、模型训练：两个参数是转移概率P(st|st-1)，生成概率P(ot|st)。有监督的训练方法，生成概率要人工标记的数据（每个状态的次数、输出、输出次数），转移概率直接语言模型；；更是用的是无监督，鲍姆-韦尔奇算法：
      - 初始模型（能产生输出序列O即可，可用转移概率生成概率均匀分布来取到），则由问题一（前向后向算法）得各输出序列概率P(O|M)，问题二（维特比算法）得产生O的路径即各路径的概率，这些路径实际可视为“标注的训练数据”，则可得一个新模型，可证明其P(O|M)更大，故完成一次迭代；；直到质量无明显提高；；属于期望值最大化过程（EM过程），一定收敛到局部最优
  - 5.4 小结：和几乎所有的机器学习的模型工具一样， 它需要一个训练算法（鲍姆－ 卡尔奇算法）和使用时的解码算法（维特比算法），掌握了这两类算法，就基本上可以使用隐含马尔可夫模型这个工具了。
- 6 信息的度量和作用：1948 香农提“信息熵”，度量信息，量化其作用
  - 6.1 信息熵：`信息量`等于不确定性的多少；；谓`信息熵`，$H(x)=-ΣP(x)logP(x)$，变量不确定性↑熵↑，搞清楚需要的信'量↑，似热学熵；；单位`比特`，几比特可定此信息（如一个汉字）；；`冗余度`，一书重复多则冗'度大信'量小；
  - 6.2 信息的作用：消除不确定性；；事物内部随机性U，外部欲消之唯引信息I，新U'=U-I；；相关的信息の`条件熵`，引入条件Y，$H(X|Y)=-Σ P(x,y) logP(x|y)$，<=H(X)，等号当信息无关；；
  - 6.3 延伸阅读——信息论在信息处理中的应用
    - `互信息`：变量ﾉ“相关性”；；$I(X;Y)=Σ P(x,y) log(P(x,y)/P(x)P(y))$，=$H(X)-H(X|Y)$，即了解一个可消除另一的多少不确定性；；可解决翻译二义性（雅让斯基）；；
    - `相对熵`（か`交叉熵`）：正值函数ﾉ“相似性”；；$KL(f()||g())=Σ f() log(f()/g())$，KL↑差异↑，可用于两个随机分布；；`语言模型复杂度`（贾里尼克），已知上下文，每位置平均可选词数；；
  - 6.4 小结：热力学熵度量系统无序性
- 7 贾里尼克和现代语言处理：其作为普通人的故事
  - 7.1 早年生活：对少年时教育的看法——①中小学重社会生活志向②大学理解力强容易弥补中小学所未学③不断读书的动力④书本可晚学而成长阶段错过无法弥补；；理想不断改变，通过努力走向成功的志向不变；；大师指点故研究境界高；；对语言学家的坏印象
  - 7.2 从水门事件到莫妮卡·莱温斯基：（IBM 时期）；；阵容强大；；宽松的环境，提出统计语音识别的框架结构（当作通信问题而非人'智'和模'匹'问题）；；机读语料用电传文本；；历史必然性——①唯IBM有计算功能和数据②贾等有研究且正在IBM③小沃森发展业务至顶点；；足够好的数学基础是贾等挑选科学家的必要条件；；BCJR算法；；统计语处之所以诞生在IBM，除了因没有基础而不受条框束缚，也有历史必然性
  - 7.3 一位老人的奇迹：约翰霍普金斯大学CLSP；；什么方法不好；；学习是一辈子的事。
- 8 布尔代数和搜索引擎的索引
  - 搜索引擎的基本服务：下载、索引、排序
  - 8.1 布尔代数：数学方法解决逻辑问题；；真值表即可；；
  - 8.2 索引：最简单是长二进制数表示一关键字在否各文（网页）；；词汇表大小×网页数，必须分布式存于些服务器，常见做法是索引分许多份（shard），复又根据重要性、质量、访问频率建不同级别索引，常用索引访问快信息多更新快；；无论如何复杂原理等价于布尔运算；；
  - 8.3 小结：“( 人们）发觉真理在形式上从来是简单的，而不是复杂和含混的。”(Trnth is ever to be found in simplicity, and not in the multiplicity and confusion of things.)
- 9 图论和网络爬虫：搜索引擎的下载；；离散数学ﾊ数理逻辑、集合论、图论、近世代数
  - 9.1 图论：广搜深搜，都记录已访；；
  - 9.2 网络爬虫：互联网ﾊ图，网页ﾊ节点，超链接ﾊ弧，网络爬虫ﾊ遍历而存者，哈希表记已访；；商业的网络爬虫要有成千上万个服务器，以高速网络连接
  - 9.3 延伸阅读——图论的两点补充说明：
    - 9.3.1 欧拉七桥问题的证明：若不重复遍历而回原点，每顶点的度必偶数
    - 9.3.2 构建网络爬虫的工程要点：管理下载优先级排序的调度系统(Scheduler) ，工程上更像广搜，（由于握手时间）下载一个网站再另一个，像深搜，总体广搜成分更多；；页面分析和URL提取，解析程序；；URL表，存储哈希表的服务器成了爬虫系统的瓶颈，好的方法一般都，明确分工放判断，下载询问和更新用批处理
  - 9.4 小结：很多数学方法就是这样，看上去没有什么实际用途，但是随疗时间的推移会一下子派上大用场。这恐怕是世界上还有很多人毕生研究数学的原因， 
- 10 PageRank - Google 的民主表决式网页排名技术：这两章为排序，本章为质量（Quality），下一章关键词与网页的相关性（Relevance）
  - 10.1 PageRank 算法的原理：链接数量；；被链接更多则排名高，排名高的表决权大；；二维矩阵相乘问题，迭代解决（相同初始值），布林与佩奇证明其收敛；；稀疏矩阵计算的技巧；；MapReduce 使其并行计算自动化；；互联网ﾊ图；；网页排名的高明之处在于它把整个互联网当作一个整体来对待。这无意识中符合了系统论的观点；；
  - 10.2 延伸阅读——PageRank 的计算方法：方阵点乘列向量，$B_i=A·B_{i-1}$，一般10次收敛，用很小的常数平滑之（对零概率小概率平滑处理）
  - 10.3 小结：算法受专利保护，使斯坦福有谷歌超1%股票
- 11 网页和查询的相关性
  - 11.1 搜索关键词权重的科学度量 TF-IDF：单文本词频，逆文本频率指数，相乘；；IDF即是特定条件下关键词概率分布的交叉熵
  - 11.2 延伸阅读——TF-IDF的信息论依据
- 12 地图和本地搜索的最基本技术 - 有限状态机和动态规划