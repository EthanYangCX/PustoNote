ProbStat | 概率论、统计学、随机过程

CLC: O21

随机变量、贝叶斯法则、独立和条件独立、伯努利分布、高斯分布、链式法则

一些杂散知识

- 模型拟合
    - AIC（赤池信息准则，Akaike information criterion），是衡量统计模型拟合优良性(Goodness of fit)的一种标准。赤池信息准则的方法是寻找可以最好地解释数据但包含最少自由参数的模型。找出最小AIC值相对应的模型作为选择对象。

# 统计学

在概率论中，我们所研究的随机变量，它的分布都是假设已知的，在这一前提下去研究它的性质、特点和规律性，例如求出它的数字特征，讨论随机变量函数的分布，介绍常用的各种分布等．在数理统计中，我们研究的随机变量，它的分布是未知的，或者是不完全知道的，人们是通过对所研究的随机变量进行重复独立的观察，得到许多观察值，对这些数据进行分析，从而对所研究的随机变量的分布作出种种推断的．

《白话统计学》から

- 研究对象
    - `总体（population）`
    - `样本（sample）`：抽取的其子集
    - *说明：总体不一定有大量对象；*
- 描述量
    - `参数（parameter）`：从总体数据计算并适用于总体的值
    - `统计量（statistic）`：从样本数据中计算的值
- 统计是否能推广
    - `描述统计（descriptive）`：只应用于样本对象，或总体的一些成员
    - `推断统计（inferential）`：假定样本能够代表更大的总体，故用样本推断总体特征
- 抽样方法
    - `随机抽样（random sampling）`：总体中每一个对象被选入样本的概率相等。样本与总体间的差异不是系统性的
    - `典型抽样（representative sampling）`：有意选取在具体特征上与总体匹配的对象。样本中各组的比例同于总体
    - `方便抽样（convenience sampling）`：根据地理距离、接触难度、参与意愿等方便程度来选择样本。如果与总体间的差异不至于影响研究结果，则不失为可接受的抽样方法。
- 变量类型和测量尺度
    - `常量（constant）`：具有唯一取值
    - `变量（variable）`：具有不止一个取值
        - 分类
            - `定量（quantitative）`か`连续（continuous）`：数字或评分来赋值，表示某种数量
            - `定性（qualitative）`か`分类（categorical）`：其赋值不代表特定性质的多寡，比如代表不同地区
                - `二值变量（dichotomous variable）`：具有两个不同分类
        - 测量尺度
            - `定类变量（nominally scaled variable）`：利用无权重或无数值的符号，如01代男女
            - `定序变量（ordinal variable）`：取值含权重，但不包含距离信息
            - `定距尺度（interval）`：测度的各单位间距离相等
            - `定比尺度（ratio）`：除了定距，还包含零值（如摄氏温度）
- 研究设计
    - `实验设计（experimental design）`：随机分组，组间比较
    - `相关性研究设计（correlational research design）`：只能收集若干变量的数据，统计研究
    - 精准控制自变量是得出因果关系的前提，实验设计可以控制，但受干扰，而相关性研究设计不能控制。相关性研究的优势是易于实施
- 分布和图表
    - 分布：变量取值的排列
        - 正态分布（normal distribution）、t分布（t distribution）、F分布（F distribution）、卡方分布（chi-square distribution）……
        - 特征：分布形状、取值散布程度……
    - 图表
        - 饼图、条形图（横置或纵置，纵置又称柱状图）、柱状图、叠加柱状图、气泡图……

《计量经济学》学习中

- `解释变量`、`被解释变量`：即自变量、因变量。因为自变量解释了因变量的变化。
- `异方差`：随机误差项具有不同的方差。
- `OLS`：ordinary least square，普通最小二乘法，简称最小二乘法（最小平方法），误差の平方を最小化し
- 统计参数：对样本进行统计得到的量
- `估计`か参数估计：假设θ是总体X的一个未知参数
    - 估计量（estimator）：用未给定的有限样本算出来的这个θ，记为$\hat{θ}$。是一个随机变量，比如样本的平均值、中位数都可以是估计量，随样本而变
    - 估计值（estimate）：给定具体样本后，估计量的值，也就是估计量的一个实现
    - 点估计问题：通过一次具体抽样值来估计参数θ
    - 无偏性，无偏估计量：即估计量的期望值等于θ。
        - 可以有多个，比如按不同比例把样本中的成员混合起来都是无偏估计量
    - 有效性：无偏估计量之间，方差小的更有效
    - 区间估计：置信区间，参数θ在置信区间内的概率为置信度1-α，α称为显著水平。一般是求给定置信度的置信区间。

## 测度

- 总体の参数，样本の统计量
    - 样本是总体的代表 ，样本统计量可作为总体参数的估计
    - 总体の参数
        - 取值个数 $N$
        - 均值 $μ$
        - 方差 $σ^2$
        - 标准差 $σ$
    - 样本の统计量
        - 取值个数 $n$
        - 均值 $\bar{X}$
        - 中位数 $P_{50}$
        - 方差 $s^2$
        - 标准差 $s$
    - 单个成员
        - 取值 $X$
- 中心趋势的测度
    - `均值（mean）`
    - `中位数（median）`
        - 均值受`异常值（outliers）`影响，中位数非然
    - `众数（mode）`
        - 若有多个，称为多峰（multimodal），两个谓双峰（bimodal）
    - 与正态偏态
        - 正态的均值中位数众数在同一点
        - 偏态，均值被尾巴所拉偏于中位数
- 变异程度的测度
    - `极差（range）`
    - `四分位差（interquartile range, IQR）`：第75百分位数（第三四分位数）与第25百分位数（第一四分位数）之差，即，分成容量相等的四组，四分位差含中间两组取值
    - `方差（variance）`
        - 离差平方和（SS） 除以 (总体?N:(n-1))
        - 对于样本，计算公式是除以 n-1 而不是 n，可简单理解为，计算时用了样本均值，使计算出的方差值偏小，故缩小分母来抵消影响
        - 是计算标准差的中间步骤，并且方差分析、回归等很多场景都用到这里的平方和。但由于单位不同，取值本身无助于理解分布
    - `标准差（standard deviation）`：可视为与均值间的平均离差。大概信息量最大、使用最广泛。方差的平方根也。单位与成员同。
- 集中度、变异度的综合：
    - `箱线图（boxplot）`：画出三个四分位数（25%50%75%）之位，故包含中位数、极差、四分位差

## 分布

- `正态分布（normal distribution）`か`钟形曲线（bell curve）`
    - 性质`对称（symmetrical）`、`单峰（unimodal）`（均值、中位数、众数）、`渐进（asymptotic）`（左尾右尾不触及底线）
    - 意义：推断总体，需要计算给定的假设等出现的概率，而正态分布的性质就有助于使用 `概率统计量（probability statistic）`。以确定统计量取特定值的概率。
        - 即便样本取值非正态，这些取值生成的用于统计推断的抽样分布也可能是正态
        - 非正态取值分布可能意味着样本与总体间的系统性差异
    - 很多统计场合假设取值服从正态分布，违背此假设的场合则与正态分布有关的概率不再有效
    - 与抽样的关系：根据正态分布得到的概率假设样本无偏。无偏是指样本与总体的差异不是系统性的。
- 偏态分布
    - 偏度与峰度
        - 偏度（skew）：    
            - `负偏（negatively skewed）分布`
                - 尾部趋向低端，取值聚集在高端
                - 均值被取值低的尾巴拉向负向，故曰负偏
                - 不影响中位数
            - 反之`正偏（positively skewed）`（へ）
        - 峰度（kurtosis）：分布が高度か平坦度にの形态
            - `尖峰分布（leptokurtic）`：顶点高于正态
                - 故而分布在均值附近的比例更大（如在 $μ±1σ$ 的>68%）
            - `扁峰分布（platykurtic）`

标准化与z分数

- `z分数（z score）`か`标准化值`か`标准取值（standard score）`か`标准差单位`
    - 即以标准差为单位度量的分布中一个给定取值与均值之间的距离数
    - $z = \frac{X-μ} {σ}$
    - `标准化（standardization）`：原始取值减均值除以标准差
        - 整个分布标准化后，均值0，标准差1
        - 比较不同测量单位的取值的重要工具
    - `百分位数取值（percentile scores）`：哪个取值位于分布中的第某百分位
        - 若数据服从正态分布，则可以用z分数计算。否则z分数叵用以。而应用排序的方法。

标准误（standard error）

- 定义
    - 实际是指定统计量の抽样分布の标准差。
    - 一总体から同容量样本を抽取る预期差异  を度量。故是许多推断统计量计算公式的分母
- 所有统计量都有标准误，但主要使用均值の标准误
- 抽样分布（sampling distribution）：各个样本的统计量不同，这就有了分布
    - 为了与简单频数分布区分开，其均值称为 `期望值（expected value）`，标准差称为 `标准误`
        - 称为期望，是因为均值抽样分布的均值等于总体均值，择一样本其均值的最佳猜测就是与总体均值一样
- 均值の标准误
    - 是单个样本均值与其期望的平均差异，即，用样本均值代替总体均值的确信程度（预期误差）
    - **标准差は取值と取值均值の偏差；标准误は均抽と均抽均值の偏差；又因抽样用于代而估总，名之曰误，曰估计之误差**
- 标准误の计算：不会用许多抽样来算，而是单一样本来算
    - 样本标准差大，总体抽样偏差亦可望大；样本容量大，偏差可望小，故。
    - $σ_{\bar{X}} = \frac{σ} {\sqrt{n}}$
    - $s_{\bar{X}} = \frac{s} {\sqrt{n}}$
- `中心极限定理（central limit theorem）`：只要样本容量足够大，即使样本取值分布不是正态分布，均值的抽样分布也是正态的
- t分布、t值
    - `t分布族`：与正态分布相似，但形状受样本容量影响，大样本下几乎同正态分布，小样本下中间更平坦，两遍更粗厚。对称。
    - 也就是，样本均值取值的分布
    - $t = \frac {\bar{X} - μ} {s_{\bar{X}}}$
        - 与z类似
    - 常用于 推断な统计量，样本统计值相对于抽样预期差异的大小を测度
- 用标准误确定样本统计值的出现概率，可类比于，用标准分数确定个别取值的概率

## 统计显著性、效应量、置信区间

- 统计显著性（statistical significance）：统计量代表总体中某真实现象的概率。即，样本中的发现是否同样存在于总体
    - （随记：还有一种显著？样本的均值和总体的差异是否足够显著，以至于不可能只是碰巧。）
    - `随机抽样误差（random sampling error）`か`随机机会（random chance）`：样本的统计量取一定值的概率
    - `p值（p value）`（p意为概率）：计算t值，并根据样本容量得自由度，以二者查t分布表，得到t值≥此t值之概率。**胜t之概**
    - `零假设（null hypothesis）` $H_0$：假设效应不存在，即，假设样本均值与总体无何不同（$H_0: μ = \bar{X}$）
    - `替代假设（alternative hypothesis）` $H_1$ 或 $H_A$
        - `双尾（two-tailed）替代假设`：只假设不同，而不假设哪方大
        - `单尾（one-tailed）替代假设`：定向的，即假设了哪方更大
    - `α水平（alpha level）`か`第I类错误（Type I error）`：样本均值与总体多不同才认为差异并非偶然
- 效应量（effect size）
- 置信区间（confidence interval）

## 相关性

pandas

- DataFrame.corr()
    - `method =` kommt aus  {'pearson', 'kendall', 'spearman'}

- 资料
    - [聊聊统计学三大相关性系数](https://mp.weixin.qq.com/s/7mLvwssuR4W1kyaenrL5PA)
        - 三大相关性系数 Pearson、Spearman、Kendall
    - [python 计算两个列表的相关系数](https://blog.csdn.net/liuchengzimozigreat/article/details/82989224)
    - [相关性系数介绍+python代码实现 correlation analysis](https://blog.csdn.net/qq_30138291/article/details/79801777)
    - [Kendall相关系数详解-案例版](https://www.jianshu.com/p/93fd5ab408ae)
    - [Kendall rank correlation coefficient](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient)
    - [Kendall tau rank correlation coefficient](http://www.wikidoc.org/index.php/Kendall_tau_rank_correlation_coefficient)

## 假设检验

## 回归

回归的分类

- 线性回归
- 线性概率模型（LPM, linear probability model）：带有二值因变量的多元线性回归模型
- 对数几率回归（逻辑回归）
- probit 回归：零一变量，其概率服从正态分布，与logit的不同之处是logit服从对数几率分布
    - 当因变量是名义变量时，Logit和Probit没有本质的区别，一般情况下可以换用。区别在于采用的分布函数不同，前者假设随机变量服从逻辑概率分布，而后者假设随机变量服从正态分布。其实，这两种分布函数的公式很相似，函数值相差也并不大，唯一的区别在于逻辑概率分布函数的尾巴比正态分布粗一些。但是，如果因变量是序次变量，回归时只能用有序Probit模型。有序Probit可以看作是Logit的扩展。
- tobit 回归：被解释变量取值有限制、存在选择行为的这类模型。有上下界，所以跟二值有相似之处。

回归方程的显著性检验

- 用t 检验来检验回归系数的显著性
- F检验用于检验回归方程的显著性（方差分析）
- 相关系数的显著性检验（因为一元线性回归方程讨论的是变量 x 与变量 y 之间的线性关系，所以变量 x 与 y 之间的相关系数来检验回归方程的显著性。用相关系数来反应 x 与 y 的线性关系的密切程度。）
    - 对于一元线性回归来说，回归系数显著性的 t 检验，回归方程显著性的 F 检验，相关系数显著性的 t 检验，这三种检验是等价的。相关系数显著性的 t 检验与回归系数显著性的 t 检验是完全相等的，式（12）F统计量则是这两个 t 统计量的平方。对于一元线性回归只需要做一种检验即可，而对于多元线性回归，这三种检验考虑的问题不同，是三种不同的检验，并不等价。
- 样本决定系数

回归方程的

- 多重共线性（Multicollinearity）是指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。
- 方差膨胀系数(variance inflation factor，VIF)，是衡量多元线性回归模型中复 (多重)共线性严重程度的一种度量。它表示回归系数估计量的方差与假设自变量间不线性相关时方差相比的比值。（一个例子是，小于10说明不存在多重共线性）

参考：

- [从统计学看线性回归（2）——一元线性回归方程的显著性检验](https://www.cnblogs.com/datamining-bio/p/9502033.html)